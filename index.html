<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yatian Pang</title>
  
  <meta name="author" content="Yatian Pang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>
<!--
<div style="position: relative; right: 370px; text-align:right;">
  <small><em><q>It's about a man who has nothing, who risks everything, to feel something.</q> --Jessica Day</em></small>
</div>
-->

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Yatian Pang (庞雅天)</name>
              </p>
              <p>
                Greetings! My name is Yatian Pang.
                I'm a final-year PhD student at the National University of Singapore supervised by Prof.  <a href="https://scholar.google.com/citations?user=mfH9UFIAAAAJ&hl=en">Tay E. H. Francis</a>  and working closely with Prof. <a href="https://yuanli2333.github.io/">Li Yuan</a> from PKU. 
                I'm also a research intern at the Qwen-VL team, now working on Qwen3-VL with <a href="https://scholar.google.com/citations?user=ylhI1JsAAAAJ&hl">Shuai Bai</a>.
                
                   <!-- I also work with Prof. <a href="https://sites.google.com/site/sernam/">Ser-Nam Lim</a> from UCF and Prof. <a href="https://leehomyc.github.io/">Harry Yang</a> from HKUST.  -->
              </p>
              <p>
                My research focuses on multi-modal understanding and generation.
              </p>
              <p>
                <FONT COLOR="#ff0000">I am looking for a full-time position starting in 2026. Work location could be USA/Singapore/China. Feel free to drop me an email at yatian_pang@u.nus.edu if you're interested.</FONT>
              </p>
              <p style="text-align:center">
                <a href="mailto:yatian_pang@u.nus.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=AZQyNWkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Pang-Yatian">Github</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/YujunPeiyangShi">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/yatian-pang-64b5a2232/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/pyt.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pyt.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Selected Works</heading>
            </td>
          </tr>
          </tbody>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</papertitle>
            <br>
            Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, <strong>Yatian Pang</strong>, Li Yuan
 
            <br>
            arXiv, 2025
            <br>
            [<a href="https://arxiv.org/abs/2506.03147">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/UniWorld-V1">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=UniWorld-V1&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              An open-source framework to reproduce sora, a diffusion based video generation method.
            </p> -->
          </td>
        </table>  


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Next Patch Prediction for Autoregressive Visual Generation</papertitle>
            <br>
            <strong>Yatian Pang</strong>, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis E. H. Tay, Ser-Nam Lim, Harry Yang, Li Yuan
 
            <br>
            arXiv, 2024
            <br>
            [<a href="https://arxiv.org/abs/2412.15321">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/Next-Patch-Prediction">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=Next-Patch-Prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              An open-source framework to reproduce sora, a diffusion based video generation method.
            </p> -->
          </td>
        </table>  



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Open-Sora Plan: Open-Source Large Video Generation Model</papertitle>
            <br>
            Open-Sora-Plan Team
 
            <br>
            arXiv, 2024
            <br>
            [<a href="https://arxiv.org/abs/2412.00131">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=Open-Sora-Plan&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              An open-source framework to reproduce sora, a diffusion based video generation method.
            </p> -->
          </td>
        </table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Languagebind: Extending video-language pretraining to n-modality by language-based
              semantic alignment </papertitle>
            <br>
            Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, <strong>Yatian Pang</strong>, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan
 
            <br>
            ICLR, 2024
            <br>
            [<a href="https://arxiv.org/abs/2310.01852">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/LanguageBind">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=LanguageBind&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              A multi-modal pretraining framework.
            </p> -->
          </td>
        </table>  


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models </papertitle>
            <br>
            Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, JunwuvZhang, <strong>Yatian Pang</strong> , Munan Ning, Li Yuan
            <br>
            arXiv, 2024
            <br>
            [<a href="https://arxiv.org/abs/2401.15947">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/MoE-LLaVA">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=MoE-LLaVA&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              A multi-modal pretraining framework.
            </p> -->
          </td>
        </table>  


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Masked autoencoders for point cloud self-supervised learning</papertitle>
            <br>
            <strong>Yatian Pang</strong>, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, Li Yuan
 
            <br>
            ECCV, 2022
            <br>
            [<a href="https://arxiv.org/abs/2203.06604">arXiv link</a>]
            [<a href="https://github.com/Pang-Yatian/Point-MAE">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=Pang-Yatian&repo=Point-MAE&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p></p>
            <!-- <p>
              A simple masked autoencoder framework for self-supervised learning on point cloud.
            </p> -->
          </td>
        </table>  


        <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
            </td>
          </tr>
          </tbody>
        </tbody></table>

        <table table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody>
              <td width="15%">
                <img src="images/nus.png" width="80">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>National University of Singapore</strong>, Singapore <br>
                <br>
                Ph.D. <br>
              </td>
            </tr> 

     
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Invited Talk</heading>
            <br>
            <span style="color: #ff0000">I'm always more than willing to give a talk about my works and my field of study!</span>
            <span style="color: #ff0000">Feel free to reach out :)</span>
            <br><br>
            <strong>Introduction to Federated Learning</strong> @ UMass Amherst, host: Hong Yu
            <br>
            <strong>Continual Learning via Bit-Level Information Preserving</strong> @ ContinualAI, host: Vincenzo Lomonaco
            <br>
          </td>
        </table> -->

        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
          <tbody><tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
                <a href="https://jonbarron.info/">template from here</a>
              </font>
            </p>
            </td>
          </tr>
          </tbody>
        </table>        
      </td>
    </tr>
  </table>
</body>

</html>
