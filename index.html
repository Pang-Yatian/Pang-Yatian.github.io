<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yatian Pang</title>
  
  <meta name="author" content="Yatian Pang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    .tag-understanding { color: #2E8B57; font-weight: bold; } /* Sea green (unchanged) */
    .tag-generation { color: #D2691E; font-weight: bold; }      /* Chocolate brown (warm, distinct from blue) */
    .tag-unify { color: #9370DB; font-weight: bold; }          /* Dodger blue */
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Yatian Pang (庞雅天)</name>
              </p>
              <p>
                Greetings! My name is Yatian Pang. I'm a final-year PhD student at the National University of Singapore supervised by Prof. <a href="https://scholar.google.com/citations?user=mfH9UFIAAAAJ&hl=en">Tay E. H. Francis</a> and working closely with Prof. <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=en">Li Yuan</a> from PKU. I'm currently a research intern at Alibaba's Qwen team working with <a href="https://scholar.google.com/citations?user=ylhI1JsAAAAJ&hl"> Shuai Bai</a> and Dr. <a href="https://scholar.google.com.hk/citations?user=kMpV724AAAAJ&hl=en"> Hang Zhang</a>.
              </p>
              
              <p>
                My research interests are multi-modal understanding and generation, as well as unified models. My long-term goal is to develop general-purpose AI systems that seamlessly understand and generate content across diverse modalities.
              </p>

              <p>
                <FONT COLOR="#ff0000">I am ACTIVELY looking for a full-time position starting in 2026.</FONT> Feel free to send me an email at <FONT COLOR="#1e90ff">yatian_pang@u.nus.edu</FONT> if you're interested.
              </p>
              <p style="text-align:center">
                <a href="mailto:yatian_pang@u.nus.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=AZQyNWkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Pang-Yatian">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yatian-pang-64b5a2232/">Linkedin</a> &nbsp/&nbsp
                <!-- <a href="images/CV_Pang_Yatian__.pdf">CV</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/pyt.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pyt.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- Projects Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Projects</heading>
            </td>
          </tr>
          </tbody>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-understanding">[understanding]</span> <papertitle>Qwen-3 VL</papertitle>
            <br>
            Qwen Team, <strong>Yatian Pang</strong>
            <br>
            2025
            <br>
            
            [<a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list">Project Page</a>]
            [<a href="https://github.com/QwenLM/Qwen3-VL">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=QwenLM&repo=Qwen3-VL&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p>
              Core contributor to video understanding tasks for Qwen3-VL, especially with a focus on long video and streaming video understanding. Achieved state-of-the-art results on various benchmarks.
            </p>
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-unify">[unify]</span> <papertitle>UniWorld</papertitle>
            <br>
            Bin Lin, et al., <strong>Yatian Pang</strong>, Li Yuan
            <br>
            2025
            <br>
            [<a href="https://arxiv.org/abs/2506.03147">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/UniWorld-V1">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=UniWorld-V1&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p>
              Unified framework for image understanding, generation, and editing following GPT-4o. Solved critical challenge of connecting frozen VLMs with diffusion generators via novel semantic encoder.
            </p>
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-generation">[generation]</span> <papertitle>Open-Sora-Plan</papertitle>
            <br>
            Open-Sora-Plan Team, <strong>Yatian Pang</strong>
            <br>
            2024
            <br>
            [<a href="https://arxiv.org/abs/2412.00131">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=Open-Sora-Plan&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
            <p>
              One of the earliest open-source efforts to reproduce Sora. Contributed to high-resolution, long-duration video generation architecture and open-sourced high quality video data .
            </p>
          </td>
        </table>

        <!-- Selected Works Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Selected works</heading>
            </td>
          </tr>
          </tbody>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-understanding">[understanding]</span> <papertitle>Video Sparse Attention for Streaming Long Video Understanding</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            Submission in progress, 2025
            <br>
            [<a href="https://arxiv.org/abs/25xxxxxxx">arXiv link (coming soon)</a>]
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-unify">[unify]</span> <papertitle>Unified Autoregressive Pretraining for Image Generation and Representation Learning</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            Submission in progress, 2025
            <br>
            [<a href="https://arxiv.org/abs/25xxxxxxx">arXiv link (coming soon)</a>]
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-generation">[generation]</span> <papertitle>Next Patch Prediction for Autoregressive Visual Generation</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            arXiv, 2024
            <br>
            [<a href="https://arxiv.org/abs/2412.15321">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/Next-Patch-Prediction">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=Next-Patch-Prediction&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-generation">[generation]</span> <papertitle>DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            ICCV 2025
            <br>
            [<a href="https://arxiv.org/abs/2412.00397">arXiv link</a>]
            [<a href="https://github.com/PKU-YuanGroup/DreamDance">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=DreamDance&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
          </td>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-generation">[generation]</span> <papertitle>Envision3D: One Image to 3D with Anchor Views Interpolation</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            arXiv, 2024
            <br>
            [<a href="https://arxiv.org/abs/2403.08902">arXiv link</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=Envision3D&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
          </td>
          
        </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <span class="tag-understanding">[understanding]</span> <papertitle>Masked autoencoders for point cloud self-supervised learning</papertitle>
            <br>
            <strong>Yatian Pang</strong>, et al.
            <br>
            ECCV, 2022
            <br>
            [<a href="https://arxiv.org/abs/2203.06604">arXiv link</a>]
            [<a href="https://github.com/Pang-Yatian/Point-MAE">code</a>]
            <iframe src="https://ghbtns.com/github-btn.html?user=Pang-Yatian&repo=Point-MAE&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
          </td>
        </table>

                <!-- MoE-LLaVA -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="tag-understanding">[understanding]</span> <papertitle>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</papertitle>
                    <br>
                    Bin Lin, et al., <strong>Yatian Pang</strong>, et al.
                    <br>
                    IEEE Transactions on Multimedia (TMM), 2024
                    <br>
                    [<a href="https://arxiv.org/pdf/2401.15947">arXiv link</a>]
                    [<a href="https://github.com/PKU-YuanGroup/MoE-LLaVA">code</a>]
                    <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=MoE-LLaVA&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
<!--                     <p>
                      A large vision-language model framework using Mixture-of-Experts (MoE) architecture to improve scalability and efficiency for multi-modal tasks.
                    </p> -->
                  </td>
                </table>  
        
                <!-- LanguageBind -->
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <span class="tag-understanding">[understanding]</span> <papertitle>LanguageBind: Extending Video-Language Pretraining to N-Modality by Language-Based Semantic Alignment</papertitle>
                    <br>
                    Bin Zhu, et al., <strong>Yatian Pang</strong>, et al.
                    <br>
                    International Conference on Learning Representations (ICLR) 2024
                    <br>
                    [<a href="https://arxiv.org/abs/2310.01852">arXiv link</a>]
                    [<a href="https://github.com/PKU-YuanGroup/LanguageBind">code</a>]
                    <iframe src="https://ghbtns.com/github-btn.html?user=PKU-YuanGroup&repo=LanguageBind&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe>
<!--                     <p>
                      A multi-modal pretraining framework that unifies video, language, and additional modalities through language-based semantic alignment for cross-modal understanding.
                    </p> -->
                  </td>
                </table>  


        <!-- Education Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody><tr>
            <td width="100%" valign="middle">
              <heading>Education</heading>
            </td>
          </tr>
          </tbody>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tbody>
              <tr>
                <td width="15%">
                  <img src="images/nus.png" width="80">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <strong>National University of Singapore</strong>, Singapore <br>
                  Ph.D. <br>
                  Aug 2021 - Present <br>
                  Advisor: Prof. Tay Eng Hock, Francis <br>
                  Collaborator: Prof. Li Yuan (Peking University)
                </td>
              </tr>
        </tbody></table>

        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
          <tbody><tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
                <a href="https://jonbarron.info/">template from here</a>
              </font>
            </p>
            </td>
          </tr>
          </tbody>
        </table>

        <script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffffff&w=399&t=m&d=FsshZJr1BrBWdKxo6OuxTTcU7I1Ih5DuDuYbt1u_bNE&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script>
  
      </td>
    </tr>
  </table>
</body>

</html>
